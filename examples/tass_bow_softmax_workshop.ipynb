{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f63953b",
   "metadata": {},
   "source": [
    "# Taller: Análisis de Sentimientos en Tweets en Español (TASS 2018)\n",
    "\n",
    "En este notebook vamos a construir paso a paso un clasificador de sentimiento para tweets en español utilizando el corpus de **TASS 2018**.  \n",
    "El objetivo pedagógico es entender cada etapa típica de un *pipeline* de NLP aplicado a clasificación de texto:\n",
    "\n",
    "1. Contexto del problema y del corpus TASS.\n",
    "2. Configuración del entorno de trabajo.\n",
    "3. Carga y exploración de los datos.\n",
    "4. Preprocesamiento de texto en español.\n",
    "5. Representación mediante **Bolsa de Palabras (Bag of Words)**.\n",
    "6. Manejo del desbalance de clases.\n",
    "7. Definición y entrenamiento de un modelo **Softmax (Regresión Logística Multinomial)**.\n",
    "8. Evaluación con métricas de clasificación y matriz de confusión.\n",
    "\n",
    "Cada bloque de código irá precedido de una breve explicación en español para que el flujo completo sea fácil de seguir en el taller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f378ee8",
   "metadata": {},
   "source": [
    "## 1. Configuración del entorno\n",
    "\n",
    "En este bloque definimos la ruta donde se encuentran los datos de TASS y añadimos el directorio actual al `sys.path` para poder importar módulos locales (por ejemplo, la clase `TextProcessing` que usaremos para el preprocesamiento).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e301715d-b508-44f1-8f4b-96e0c57488cb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\epuerta\\\\OneDrive - Universidad Tecnológica de Bolívar\\\\Apps\\\\courseNLP\\\\examples\\\\data\\\\tass\\\\'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "PATH = os.getcwd()\n",
    "DIR_DATA = PATH + '{0}data{0}tass{0}'.format(os.sep)\n",
    "sys.path.append(PATH) if PATH not in list(sys.path) else None\n",
    "DIR_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d666f5",
   "metadata": {},
   "source": [
    "## 2. Contexto: TASS 2018\n",
    "\n",
    "**TASS** es una campaña de evaluación y taller científico centrado en el **análisis de sentimientos en Twitter en español**.  \n",
    "En la edición 2018, una de las tareas principales consiste en predecir la **polaridad global** de cada tweet (por ejemplo, positiva, negativa, neutra, etc.) a partir de texto corto, ruidoso y con variantes dialectales.\n",
    "\n",
    "En este taller utilizaremos una versión de ese corpus para entrenar un modelo supervisado de clasificación de sentimiento.\n",
    "\n",
    "Más información: <http://tass.sepln.org/2018/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4844cb",
   "metadata": {},
   "source": [
    "## 3. Importación de librerías\n",
    "\n",
    "En el siguiente bloque importamos las librerías necesarias para:\n",
    "\n",
    "- Manipular datos (`pandas`, `numpy`).\n",
    "- Visualizar resultados (`matplotlib`, `seaborn`).\n",
    "- Preprocesar y transformar texto (`TextProcessing`, `CountVectorizer`).\n",
    "- Construir y evaluar modelos (`LogisticRegression`, métricas de `sklearn`).\n",
    "- Tratar el desbalance de clases (`RandomOverSampler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df52b8f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-21T03:05:35.137355Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from logic.text_processing import TextProcessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a38d39",
   "metadata": {},
   "source": [
    "## 4. Inicialización de utilidades de preprocesamiento\n",
    "\n",
    "Instanciamos:\n",
    "\n",
    "- `TextProcessing()`: encapsula las transformaciones de texto (limpieza, normalización, etc.).\n",
    "- `LabelEncoder()`: utilidad para mapear etiquetas de texto a códigos numéricos si fuera necesario.\n",
    "\n",
    "Esto nos permite mantener el preprocesamiento separado de la lógica del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191fa4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Text Processing\n",
      "es: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "tp = TextProcessing()\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2566a",
   "metadata": {},
   "source": [
    "## 5. Carga de los conjuntos de entrenamiento y prueba\n",
    "\n",
    "A continuación cargamos:\n",
    "\n",
    "- **`tass2018_es_train.csv`**: tweets etiquetados con su polaridad (conjunto de entrenamiento).\n",
    "- **`tass2018_es_test.csv`**: tweets separados para evaluación final.\n",
    "\n",
    "Es importante que durante el taller se explique el formato de las columnas (por ejemplo: identificador, contenido del tweet, etiqueta de polaridad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b3ebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment/polarity/value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768213876278165504</td>\n",
       "      <td>OnceBukowski</td>\n",
       "      <td>-Me caes muy bien \\r\\n-Tienes que jugar más pa...</td>\n",
       "      <td>2016-08-23 22:30:35</td>\n",
       "      <td>es</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768213567418036224</td>\n",
       "      <td>anahorxn</td>\n",
       "      <td>@myendlesshazza a. que puto mal escribo\\r\\n\\r\\...</td>\n",
       "      <td>2016-08-23 22:29:21</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768212591105703936</td>\n",
       "      <td>martitarey13</td>\n",
       "      <td>@estherct209 jajajaja la tuya y la d mucha gen...</td>\n",
       "      <td>2016-08-23 22:25:29</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768221670255493120</td>\n",
       "      <td>endlessmilerr</td>\n",
       "      <td>Quiero mogollón a @AlbaBenito99 pero sobretodo...</td>\n",
       "      <td>2016-08-23 23:01:33</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768221021300264964</td>\n",
       "      <td>JunoWTFL</td>\n",
       "      <td>Vale he visto la tia bebiendose su regla y me ...</td>\n",
       "      <td>2016-08-23 22:58:58</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid           user  \\\n",
       "0  768213876278165504   OnceBukowski   \n",
       "1  768213567418036224       anahorxn   \n",
       "2  768212591105703936   martitarey13   \n",
       "3  768221670255493120  endlessmilerr   \n",
       "4  768221021300264964       JunoWTFL   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  -Me caes muy bien \\r\\n-Tienes que jugar más pa...  2016-08-23 22:30:35   \n",
       "1  @myendlesshazza a. que puto mal escribo\\r\\n\\r\\...  2016-08-23 22:29:21   \n",
       "2  @estherct209 jajajaja la tuya y la d mucha gen...  2016-08-23 22:25:29   \n",
       "3  Quiero mogollón a @AlbaBenito99 pero sobretodo...  2016-08-23 23:01:33   \n",
       "4  Vale he visto la tia bebiendose su regla y me ...  2016-08-23 22:58:58   \n",
       "\n",
       "  lang sentiment/polarity/value  \n",
       "0   es                     NONE  \n",
       "1   es                        N  \n",
       "2   es                        N  \n",
       "3   es                        P  \n",
       "4   es                        N  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(DIR_DATA + 'tass2018_es_train.csv', sep=',')\n",
    "data_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc97fe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment/polarity/value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>770976639173951488</td>\n",
       "      <td>noseashetero</td>\n",
       "      <td>@noseashetero 1000/10 de verdad a ti que voy a...</td>\n",
       "      <td>2016-08-31 13:28:49</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>771092421866389508</td>\n",
       "      <td>Templelx</td>\n",
       "      <td>@piscolabisaereo @HistoriaNG @SPosteguillo las...</td>\n",
       "      <td>2016-08-31 21:08:54</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>771092111429083136</td>\n",
       "      <td>esskuu94</td>\n",
       "      <td>Al final han sido 3h  Bueno, mañana tengo fies...</td>\n",
       "      <td>2016-08-31 21:07:40</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>771092070572449796</td>\n",
       "      <td>__ariadna9</td>\n",
       "      <td>@Jorge_Ruiz14 yo no tengo tiempo para esas cos...</td>\n",
       "      <td>2016-08-31 21:07:30</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>771094192508600320</td>\n",
       "      <td>_cristtina15_</td>\n",
       "      <td>@_MissChaotic_ ves ese brillo? es un coso que ...</td>\n",
       "      <td>2016-08-31 21:15:56</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid           user  \\\n",
       "0  770976639173951488   noseashetero   \n",
       "1  771092421866389508       Templelx   \n",
       "2  771092111429083136       esskuu94   \n",
       "3  771092070572449796     __ariadna9   \n",
       "4  771094192508600320  _cristtina15_   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  @noseashetero 1000/10 de verdad a ti que voy a...  2016-08-31 13:28:49   \n",
       "1  @piscolabisaereo @HistoriaNG @SPosteguillo las...  2016-08-31 21:08:54   \n",
       "2  Al final han sido 3h  Bueno, mañana tengo fies...  2016-08-31 21:07:40   \n",
       "3  @Jorge_Ruiz14 yo no tengo tiempo para esas cos...  2016-08-31 21:07:30   \n",
       "4  @_MissChaotic_ ves ese brillo? es un coso que ...  2016-08-31 21:15:56   \n",
       "\n",
       "  lang sentiment/polarity/value  \n",
       "0   es                        P  \n",
       "1   es                        P  \n",
       "2   es                        P  \n",
       "3   es                        N  \n",
       "4   es                        N  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv(DIR_DATA + 'tass2018_es_test.csv', sep=',')\n",
    "data_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b697c",
   "metadata": {},
   "source": [
    "## 6. Preprocesamiento de texto\n",
    "\n",
    "En esta etapa:\n",
    "\n",
    "1. Tomamos el texto bruto de cada tweet (`content`).\n",
    "2. Aplicamos `tp.transformer(...)` para:\n",
    "   - Normalizar el texto (minúsculas, etc.).\n",
    "   - Eliminar ruido típico de Twitter (URLs, menciones, signos repetidos, etc.).\n",
    "   - Opcionalmente, manejar tildes, emojis o risas.\n",
    "\n",
    "Guardamos:\n",
    "- `x_train`, `x_test`: listas de tweets ya preprocesados.\n",
    "- `y_train`, `y_test`: etiquetas de polaridad correspondientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace549f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 1008)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = [tp.transformer(row) for row in data_train['content'].tolist()]\n",
    "#y_train = le.fit_transform(data_train['sentiment/polarity/value'])\n",
    "y_train = data_train['sentiment/polarity/value']\n",
    "len(x_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d9b7be0-5962-406f-93d9-31b03dab0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9945c0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 506)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = [tp.transformer(row) for row in data_test['content'].tolist()]\n",
    "#y_test = le.fit_transform(data_test['sentiment/polarity/value'])\n",
    "y_test = data_test['sentiment/polarity/value']\n",
    "len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924832d",
   "metadata": {},
   "source": [
    "## 7. Representación: Bolsa de Palabras (Bag of Words)\n",
    "\n",
    "El modelo no puede trabajar directamente con texto, así que lo convertimos en vectores numéricos.\n",
    "\n",
    "Usamos `CountVectorizer` con:\n",
    "\n",
    "- `analyzer='word'`\n",
    "- `ngram_range=(1, 3)` para incluir unigramas, bigramas y trigramas.\n",
    "\n",
    "Cada tweet se representa como un vector donde cada posición corresponde a una palabra o n-grama del vocabulario y el valor es su frecuencia en el tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9220c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(analyzer='word', ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f397be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = bow.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d8a423e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(1008, 25996))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd8deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = bow.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87edb6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(506, 25996))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2d2e9",
   "metadata": {},
   "source": [
    "## 8. Análisis de distribución de clases\n",
    "\n",
    "Antes de entrenar, observamos cuántos ejemplos hay por clase en entrenamiento y prueba.  \n",
    "Esto nos permite detectar si el conjunto está **desbalanceado** (por ejemplo, muchas más opiniones neutrales que negativas), algo habitual en tareas reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "986c24e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sample train: [('N', 418), ('NEU', 133), ('NONE', 139), ('P', 318)]\n"
     ]
    }
   ],
   "source": [
    "print('**Sample train:', sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d3c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sample test: [('N', 219), ('NEU', 69), ('NONE', 62), ('P', 156)]\n"
     ]
    }
   ],
   "source": [
    "print('**Sample test:', sorted(Counter(y_test).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567c35c",
   "metadata": {},
   "source": [
    "## 9. Esquema de validación: ShuffleSplit (Validación Cruzada)\n",
    "\n",
    "Para estimar el rendimiento del modelo de forma más robusta usamos `ShuffleSplit`:\n",
    "\n",
    "- Se generan varias particiones aleatorias (aquí, 10).\n",
    "- En cada partición se entrena con una parte de los datos y se evalúa con el resto.\n",
    "\n",
    "Esto ayuda a reducir la dependencia de una única partición entrenamiento/prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb45855",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = ShuffleSplit(n_splits=10, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4489060",
   "metadata": {},
   "source": [
    "## 10. Manejo del desbalance de clases: `RandomOverSampler`\n",
    "\n",
    "Si algunas clases tienen muy pocos ejemplos, el modelo tiende a ignorarlas.\n",
    "Con `RandomOverSampler`:\n",
    "\n",
    "- Replicamos ejemplos de las clases minoritarias hasta equilibrar el conjunto.\n",
    "- En este notebook se aplica tanto sobre `x_train` como sobre `x_test` para simplificar el ejercicio.\n",
    "\n",
    "**Nota didáctica:** En un escenario real, el sobremuestreo se aplica únicamente sobre los datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ba65234",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros_train = RandomOverSampler(random_state=1000)\n",
    "x_train, y_train = ros_train.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c8010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**OverSample train: [('N', 418), ('NEU', 418), ('NONE', 418), ('P', 418)]\n"
     ]
    }
   ],
   "source": [
    "print('**OverSample train:', sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b6897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros_test = RandomOverSampler(random_state=1000)\n",
    "x_test, y_test = ros_test.fit_resample(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69836dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**OverSample test: [('N', 219), ('NEU', 219), ('NONE', 219), ('P', 219)]\n"
     ]
    }
   ],
   "source": [
    "print('**OverSample test:', sorted(Counter(y_test).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c542e",
   "metadata": {},
   "source": [
    "## 11. Modelo de clasificación: Regresión Logística Multinomial (Softmax)\n",
    "\n",
    "Utilizamos `LogisticRegression` con:\n",
    "\n",
    "- `multi_class=\"multinomial\"` y `solver=\"lbfgs\"`\n",
    "\n",
    "Esto implementa un clasificador **Softmax** que aprende una probabilidad para cada clase de polaridad a partir del vector BOW del tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2364f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38867c82",
   "metadata": {},
   "source": [
    "## 12. Métricas de evaluación\n",
    "\n",
    "Vamos a registrar en cada iteración de validación cruzada:\n",
    "\n",
    "- **Accuracy**: proporción de aciertos.\n",
    "- **Recall (macro)**: capacidad de recuperar correctamente cada clase.\n",
    "- **Precision (weighted)**: precisión ponderada por soporte de cada clase.\n",
    "- **F1-score (weighted)**: equilibrio entre precisión y recall.\n",
    "\n",
    "Esto nos da una visión más completa del comportamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "246b4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_scores = []\n",
    "recalls_scores = []\n",
    "precisions_scores = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02f3dd",
   "metadata": {},
   "source": [
    "## 13. Entrenamiento y validación cruzada\n",
    "\n",
    "En el siguiente bloque:\n",
    "\n",
    "1. Generamos las particiones con `ShuffleSplit`.\n",
    "2. Entrenamos el modelo `softmax` en los datos de entrenamiento de cada partición.\n",
    "3. Predecimos sobre la parte de validación.\n",
    "4. Calculamos las métricas y las guardamos para luego promediarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a59dfd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\envs\\python-nlp\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in k_fold.split(x_train, y_train):\n",
    "    data_train = x_train[train_index]\n",
    "    target_train = y_train[train_index]\n",
    "    \n",
    "    data_test = x_train[test_index]\n",
    "    target_test = y_train[test_index]\n",
    "\n",
    "    softmax.fit(data_train, target_train)\n",
    "    predict = softmax.predict(data_test)\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(target_test, predict)\n",
    "    accuracies_scores.append(accuracy)\n",
    "    # Recall\n",
    "    recall = recall_score(target_test, predict, average='macro')\n",
    "    recalls_scores.append(recall)\n",
    "    # Precision\n",
    "    precision = precision_score(target_test, predict, average='weighted')\n",
    "    precisions_scores.append(precision)\n",
    "    # F1\n",
    "    f1 = f1_score(target_test, predict, average='weighted')\n",
    "    f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0995e",
   "metadata": {},
   "source": [
    "## 14. Resultados promedio en validación\n",
    "\n",
    "Calculamos el valor promedio de cada métrica a lo largo de las particiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddae023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_recall = round(np.mean(recalls_scores) * 100, 2)\n",
    "average_precision = round(np.mean(precisions_scores) * 100, 2)\n",
    "average_f1 = round(np.mean(f1_scores) * 100, 2)\n",
    "average_accuracy = round(np.mean(accuracies_scores) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa4602ab-2c62-468e-8ecc-7dc3c93acb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(80.24)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10c6c1",
   "metadata": {},
   "source": [
    "## 15. Evaluación final sobre el conjunto de prueba\n",
    "\n",
    "Entrenamos el modelo final y evaluamos sobre el conjunto de prueba para obtener:\n",
    "\n",
    "- **Reporte de clasificación** por clase.\n",
    "- **Matriz de confusión**, que muestra cómo se confunden las clases entre sí.\n",
    "\n",
    "Estas salidas permiten discutir en el taller:\n",
    "- Qué clases se predicen mejor o peor.\n",
    "- Cómo influye el desbalance y la representación BOW.\n",
    "- Posibles mejoras (embeddings, modelos neuronales, ajuste de parámetros, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d77705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for features in x_test:\n",
    "    features = features.reshape(1, -1)\n",
    "    value = softmax.predict(features)[0]\n",
    "    y_predict.append(value)\n",
    "\n",
    "classification = classification_report(y_test, y_predict)\n",
    "confusion = confusion_matrix(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d32b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = {'F1-score': average_f1, 'Accuracy': average_accuracy, 'Recall': average_recall, \n",
    "                 'Precision': average_precision, 'Classification Report\\n': classification, \n",
    "                 'Confusion Matrix\\n': confusion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "892ae3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score 80.21\n",
      "Accuracy 80.26\n",
      "Recall 80.24\n",
      "Precision 80.34\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           N       0.35      0.69      0.47       219\n",
      "         NEU       0.46      0.19      0.27       219\n",
      "        NONE       0.42      0.18      0.25       219\n",
      "           P       0.39      0.47      0.43       219\n",
      "\n",
      "    accuracy                           0.38       876\n",
      "   macro avg       0.41      0.38      0.35       876\n",
      "weighted avg       0.41      0.38      0.35       876\n",
      "\n",
      "Confusion Matrix\n",
      " [[151  92 109  78]\n",
      " [ 15  41  21  12]\n",
      " [ 15  14  39  25]\n",
      " [ 38  72  50 104]]\n"
     ]
    }
   ],
   "source": [
    "for item, val in output_result.items():\n",
    "    print('{0} {1}'.format(item, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5d866",
   "metadata": {},
   "source": [
    "## 16. Próximos pasos sugeridos para el taller\n",
    "\n",
    "Algunas extensiones que se pueden proponer a los participantes:\n",
    "\n",
    "- Probar distintas configuraciones de `CountVectorizer` (solo unigramas, límite de vocabulario, stopwords, etc.).\n",
    "- Comparar la Regresión Logística con otros clasificadores (SVM, árboles, redes neuronales sencillas).\n",
    "- Analizar ejemplos mal clasificados a partir de la matriz de confusión.\n",
    "- Integrar representaciones basadas en *embeddings* para comparar con la bolsa de palabras.\n",
    "\n",
    "Con estas actividades, el notebook sirve como guía completa de un pipeline clásico de NLP aplicado a TASS 2018.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
