{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVdz-TCwnfQW"
      },
      "source": [
        "# Sistema RAG para Consultas a Documentos PDF\n",
        "\n",
        "**Estudiante:** Jeison David Jiménez Alvear\n",
        "\n",
        "**Código:** T00054331\n",
        "\n",
        "**Curso:** Procesamiento del lenguaje natural\n",
        "\n",
        "**Institución:** Universidad Tecnológica de Bolívar\n",
        "\n",
        "---\n",
        "\n",
        "Este notebook implementa un sistema de Recuperación Aumentada con Generación (RAG, por sus siglas en inglés) para responder preguntas sobre el contenido de documentos PDF. El objetivo es combinar la recuperación de información semántica con modelos generativos, permitiendo generar respuestas más precisas, basadas en fragmentos relevantes del documento.\n",
        "\n",
        "Este enfoque es especialmente útil cuando se necesita consultar documentos largos o técnicos de forma conversacional o automatizada.\n",
        "\n",
        "\n",
        "## 🧭 Flujo General del Proceso\n",
        "\n",
        "1. **Instalación de dependencias**\n",
        "   - Se instalan las bibliotecas necesarias como `PyMuPDF`, `OpenAI`, `FAISS` y `scikit-learn`.\n",
        "\n",
        "2. **Extracción de texto desde archivos PDF** (`extract_text_from_pdf`)\n",
        "   - Se define una función que lee el contenido textual de un PDF utilizando `PyMuPDF`.\n",
        "\n",
        "3. **Fragmentación del texto (chunking)** (`extract_and_chunk_text_from_pdf`)\n",
        "   - El texto extraído se divide en fragmentos con solapamiento para preservar el contexto y mejorar la recuperación.\n",
        "\n",
        "4. **Creación del índice vectorial** (`create_index`)\n",
        "   - Se convierten los fragmentos en vectores usando embeddings y se indexan con FAISS, lo que permite realizar búsquedas semánticas eficientes.\n",
        "\n",
        "5. **Búsqueda de información relevante** (`search_index`)\n",
        "   - Ante una pregunta del usuario, se busca en el índice FAISS los fragmentos más relevantes relacionados con la consulta.\n",
        "\n",
        "6. **Generación de respuestas** (`generate_answer`)\n",
        "   - Se utiliza un modelo generativo (como GPT) para generar una respuesta basada en los fragmentos recuperados, proporcionando una salida contextualizada y útil.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d13m3w6N4We",
        "outputId": "4148b836-7acc-42d4-a816-16e5268bc362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0 pymupdf-1.25.5\n"
          ]
        }
      ],
      "source": [
        "!pip install openai pymupdf faiss-cpu scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hK0dYUfbN0JS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Sistema RAG (Retrieval-Augmented Generation) para consultas sobre documentos PDF.\n",
        "Este script permite cargar documentos PDF, indexarlos y generar respuestas\n",
        "enriquecidas utilizando un modelo de lenguaje.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from google.colab import files\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KQfkH2C0OMrK"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extrae texto de un archivo PDF.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Ruta al archivo PDF\n",
        "\n",
        "    Returns:\n",
        "        str: Texto extraído del PDF\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pnnsz9KGOSlo"
      },
      "outputs": [],
      "source": [
        "def extract_and_chunk_text_from_pdf(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Extrae texto de un PDF y lo divide en fragmentos más pequeños (chunks)\n",
        "    con cierto solapamiento para mantener el contexto.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Ruta al archivo PDF\n",
        "        chunk_size (int): Tamaño aproximado de cada fragmento en caracteres\n",
        "        chunk_overlap (int): Cantidad de caracteres que se solapan entre fragmentos\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de fragmentos de texto con metadatos\n",
        "    \"\"\"\n",
        "    # Extraer texto completo\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        full_text += page.get_text()\n",
        "\n",
        "    # Dividir en chunks\n",
        "    chunks = []\n",
        "    doc_name = os.path.basename(pdf_path)\n",
        "\n",
        "    # Podemos dividir por párrafos para mantener coherencia\n",
        "    paragraphs = full_text.split('\\n\\n')\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        paragraph = paragraph.strip()\n",
        "        if not paragraph:\n",
        "            continue\n",
        "\n",
        "        # Si añadir este párrafo excede el tamaño del chunk, guardamos el chunk actual\n",
        "        if len(current_chunk) + len(paragraph) > chunk_size and current_chunk:\n",
        "            chunks.append({\n",
        "                \"text\": current_chunk,\n",
        "                \"source\": doc_name,\n",
        "                \"chunk_id\": len(chunks)\n",
        "            })\n",
        "            # Mantener algo del texto anterior para preservar contexto\n",
        "            current_chunk = current_chunk[-chunk_overlap:] if chunk_overlap > 0 else \"\"\n",
        "\n",
        "        # Añadir el párrafo al chunk actual\n",
        "        if current_chunk:\n",
        "            current_chunk += \"\\n\\n\" + paragraph\n",
        "        else:\n",
        "            current_chunk = paragraph\n",
        "\n",
        "    # Añadir el último chunk si contiene texto\n",
        "    if current_chunk:\n",
        "        chunks.append({\n",
        "            \"text\": current_chunk,\n",
        "            \"source\": doc_name,\n",
        "            \"chunk_id\": len(chunks)\n",
        "        })\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ozI0tCtwOX7O"
      },
      "outputs": [],
      "source": [
        "def create_index(documents):\n",
        "    \"\"\"\n",
        "    Crea un índice vectorial FAISS a partir de una lista de documentos.\n",
        "\n",
        "    Args:\n",
        "        documents (list): Lista de documentos de texto\n",
        "\n",
        "    Returns:\n",
        "        tuple: (índice FAISS, vectorizador TF-IDF)\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    doc_vectors = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "    # Crear índice FAISS\n",
        "    dimension = doc_vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(doc_vectors)\n",
        "\n",
        "    return index, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z89e0YbMObXS"
      },
      "outputs": [],
      "source": [
        "def search_documents(query, index, vectorizer, documents, top_k=3):\n",
        "    \"\"\"\n",
        "    Busca documentos relevantes para una consulta dada.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta del usuario\n",
        "        index (faiss.Index): Índice FAISS\n",
        "        vectorizer (TfidfVectorizer): Vectorizador TF-IDF\n",
        "        documents (list): Lista de documentos originales\n",
        "        top_k (int): Número de resultados a devolver\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (documento, puntuación)\n",
        "    \"\"\"\n",
        "    query_vector = vectorizer.transform([query]).toarray()\n",
        "\n",
        "    # Asegurarse de que top_k no sea mayor que el número de documentos\n",
        "    actual_top_k = min(top_k, len(documents))\n",
        "\n",
        "    if actual_top_k == 0:\n",
        "        print(\"ADVERTENCIA: No hay documentos disponibles para buscar.\")\n",
        "        return []\n",
        "\n",
        "    distances, indices = index.search(query_vector, actual_top_k)\n",
        "\n",
        "    # Filtrar índices inválidos\n",
        "    valid_results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        if 0 <= idx < len(documents):  # Verificar que el índice sea válido\n",
        "            valid_results.append((documents[idx], distances[0][i]))\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OJ7_KW-uSJLG"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_response(query, index, vectorizer, documents, api_key):\n",
        "    \"\"\"\n",
        "    Genera una respuesta aumentada utilizando RAG.\n",
        "\n",
        "    Args:\n",
        "        query (str): Consulta del usuario\n",
        "        index (faiss.Index): Índice FAISS\n",
        "        vectorizer (TfidfVectorizer): Vectorizador TF-IDF\n",
        "        documents (list): Lista de documentos originales\n",
        "        api_key (str): Clave API de OpenAI\n",
        "\n",
        "    Returns:\n",
        "        str: Respuesta generada\n",
        "    \"\"\"\n",
        "    # Configurar cliente de OpenAI con la nueva API\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Buscar documentos relevantes\n",
        "    search_results = search_documents(query, index, vectorizer, documents)\n",
        "\n",
        "    if not search_results:\n",
        "        return \"No se encontraron documentos relevantes para responder a tu consulta.\"\n",
        "\n",
        "    # Preparar contexto con los documentos recuperados\n",
        "    context_docs = [f\"Documento {i+1}:\\n{result[0][:1000]}\"\n",
        "                   for i, result in enumerate(search_results)]\n",
        "    context = \"\\n\\n\".join(context_docs)\n",
        "\n",
        "    # Crear un prompt estructurado para el modelo\n",
        "\n",
        "    system_prompt = (\n",
        "    \"You are a research assistant who answers questions based on \"\n",
        "    \"the provided information. Use only the information from the \"\n",
        "    \"supplied documents. If the information is insufficient to \"\n",
        "    \"answer, indicate it clearly.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "    f\"I have found these relevant document excerpts for your question:\\n\\n\"\n",
        "    f\"{context}\\n\\n\"\n",
        "    f\"Based solely on these documents, answer the following question: \"\n",
        "    f\"{query}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Generar respuesta con la nueva API\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error al generar respuesta: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUIvUMRHSLq-",
        "outputId": "6d7cb94c-fc69-4bb7-ffb1-0b0995f06b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sistema RAG para consulta de documentos PDF ===\n",
            "La carpeta /content/files no existe. Crea la carpeta y coloca allí tus archivos PDF.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Función principal del programa (versión fuera de Colab).\"\"\"\n",
        "    print(\"=== Sistema RAG para consulta de documentos PDF ===\")\n",
        "\n",
        "    # Carpeta donde están los PDFs\n",
        "    pdf_directory = \"/content/files\" #Colocar la ruta a la carpeta con los archivos .pdf\n",
        "\n",
        "    if not os.path.exists(pdf_directory):\n",
        "        print(f\"La carpeta {pdf_directory} no existe. Crea la carpeta y coloca allí tus archivos PDF.\")\n",
        "        return\n",
        "\n",
        "    # Listar los archivos PDF\n",
        "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
        "    if not pdf_files:\n",
        "        print(f\"No se encontraron archivos PDF en la carpeta {pdf_directory}.\")\n",
        "        return\n",
        "\n",
        "    # Extraer texto y fragmentar\n",
        "    all_chunks = []\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "        print(f\"Procesando {pdf_file}...\")\n",
        "        try:\n",
        "            chunks = extract_and_chunk_text_from_pdf(pdf_path)\n",
        "            all_chunks.extend(chunks)\n",
        "            print(f\"  - Fragmentos extraídos: {len(chunks)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  - Error al procesar {pdf_file}: {str(e)}\")\n",
        "\n",
        "    if not all_chunks:\n",
        "        print(\"No se pudo extraer texto de ningún archivo PDF.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Fragmentos generados ===\")\n",
        "    for i, chunk in enumerate(all_chunks[:3], 1):\n",
        "        print(f\"{i}. {chunk['source']} - {len(chunk['text'])} caracteres\")\n",
        "        preview = chunk['text'][:200].replace('\\n', ' ') + \"...\"\n",
        "        print(f\"   Vista previa: {preview}\")\n",
        "\n",
        "    chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
        "\n",
        "    print(\"\\nCreando índice de búsqueda...\")\n",
        "    index, vectorizer = create_index(chunk_texts)\n",
        "    print(f\"Índice creado con {len(chunk_texts)} fragmentos.\")\n",
        "\n",
        "    # Solicitar clave de API (opcional)\n",
        "    api_key = 'Ingresa tu api key aqui'\n",
        "\n",
        "    print(\"\\n=== Sistema listo para consultas ===\")\n",
        "    print(\"Puedes hacer preguntas sobre los documentos cargados.\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nIngresa tu consulta (o 'salir' para terminar): \")\n",
        "        if query.lower() in ['salir', 'exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        if not query.strip():\n",
        "            print(\"Por favor, ingresa una consulta válida.\")\n",
        "            continue\n",
        "\n",
        "        print(\"\\nBuscando información relevante...\")\n",
        "        try:\n",
        "            if api_key:\n",
        "                response = generate_augmented_response(query, index, vectorizer, chunk_texts, api_key)\n",
        "                print(\"\\nRespuesta:\")\n",
        "                print(response)\n",
        "            else:\n",
        "                results = search_documents(query, index, vectorizer, chunk_texts)\n",
        "                print(\"\\nFragmentos relevantes encontrados:\")\n",
        "                for i, (doc, score) in enumerate(results, 1):\n",
        "                    print(f\"\\n--- Fragmento {i} (Relevancia: {1 - score:.4f}) ---\")\n",
        "                    print(doc[:300] + \"...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar la consulta: {str(e)}\")\n",
        "\n",
        "    print(\"\\n¡Gracias por usar el sistema RAG!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Questions examples\n",
        " # 1. What does the system proposed by the SINAI group for gambling detection consist of?\n",
        " # 2. What were the performance metrics obtained by the SINAI group with its gambling detection system?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}