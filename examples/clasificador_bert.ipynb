{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c5340a-2c05-470d-b322-a43bcfcb6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f8e703-8dda-4804-aee2-c62b5680371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "PATH = os.getcwd()\n",
    "DIR_DATA = PATH + '{0}data{0}'.format(os.sep)\n",
    "sys.path.append(PATH) if PATH not in list(sys.path) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285a569e-1a98-428c-93a7-b42f08a5287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4c40a-1f00-4dcf-b83e-cc69ae744bfd",
   "metadata": {},
   "source": [
    "# Paso 1: Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520864d3-9063-44f8-a6a1-b84e54b1715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\epuerta\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments \n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff97c9-bb7e-44e8-9e56-1355a711786d",
   "metadata": {},
   "source": [
    "# Paso 2: Carga y limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28011bb3-43a9-41d3-8bcb-90979c2475f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DIR_DATA + \"dataset_sentimientos_500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767d6609-b5d8-479b-b902-04d667ce19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce302ff-4dfd-44ca-b4c8-b112d540359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Reseña', 'Sentimiento']].dropna()\n",
    "df['Sentimiento'] = df['Sentimiento'].map({'Positiva': 1, 'Negativa': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d156a2-4dd0-404e-9e4d-f772b4c460d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reseña</th>\n",
       "      <th>Sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estoy feliz con mi compra, funciona perfecto.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estoy feliz con mi compra, funciona perfecto.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recomiendo este servicio sin dudarlo.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muy recomendable, volveré a comprar.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Muy recomendable, volveré a comprar.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Muy recomendable, volveré a comprar.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>No funciona como se esperaba.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Buen precio y envío rápido.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Buen precio y envío rápido.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Una experiencia fantástica de principio a fin.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reseña  Sentimiento\n",
       "0     Estoy feliz con mi compra, funciona perfecto.            1\n",
       "1     Estoy feliz con mi compra, funciona perfecto.            1\n",
       "2             Recomiendo este servicio sin dudarlo.            1\n",
       "3              Muy recomendable, volveré a comprar.            1\n",
       "4              Muy recomendable, volveré a comprar.            1\n",
       "..                                              ...          ...\n",
       "495            Muy recomendable, volveré a comprar.            1\n",
       "496                   No funciona como se esperaba.            0\n",
       "497                     Buen precio y envío rápido.            1\n",
       "498                     Buen precio y envío rápido.            1\n",
       "499  Una experiencia fantástica de principio a fin.            1\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1864d59-6a44-472a-9f0d-f2ab0b54a8a2",
   "metadata": {},
   "source": [
    "# Paso 3: Separación en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6083b673-7e7a-4729-b9c7-73c59a8e2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "df['Reseña'].tolist(), df['Sentimiento'].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911ba49-d631-41dc-9a98-2ce4c36b62f2",
   "metadata": {},
   "source": [
    "# Paso 4: Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40abd1b6-49cd-4bd0-a2e7-ebe4ab210c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128) \n",
    "\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f4b8b-6ee9-4d15-aefd-11ad5b10dc15",
   "metadata": {},
   "source": [
    "# Paso 5: Creación de los datasets formales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff27fde1-ff12-48c5-b234-201aa1319e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({ \n",
    "\n",
    "    'input_ids': train_encodings['input_ids'], \n",
    "\n",
    "    'attention_mask': train_encodings['attention_mask'], \n",
    "\n",
    "    'labels': train_labels \n",
    "\n",
    "}) \n",
    "\n",
    "test_dataset = Dataset.from_dict({ \n",
    "\n",
    "    'input_ids': test_encodings['input_ids'], \n",
    "\n",
    "    'attention_mask': test_encodings['attention_mask'], \n",
    "\n",
    "    'labels': test_labels \n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a9c79-5878-4ed3-93b5-277da2bcc684",
   "metadata": {},
   "source": [
    "# Paso 6: Definición de métricas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bd84d2-86ff-4f47-bad6-c9859aa360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred): \n",
    "\n",
    "    logits, labels = eval_pred \n",
    "\n",
    "    preds = np.argmax(logits, axis=-1) \n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary') \n",
    "\n",
    "    acc = accuracy_score(labels, preds) \n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350e325-a8d5-48a3-a3a4-4fd8270b9ef6",
   "metadata": {},
   "source": [
    "# Paso 7: Carga del modelo preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ffae2d2-4ce7-4f6b-9096-987b9b070343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45815ef3-17a4-466a-9be0-ab17ecf447b6",
   "metadata": {},
   "source": [
    "# Paso 8: Configuración del entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b77faf6-681c-4e2d-af82-993196fdc81d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.STEPS",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      6\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      7\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      8\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      9\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     10\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1689\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m!=\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy:\n\u001b[1;32m-> 1689\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Evaluation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1692\u001b[0m         )\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m==\u001b[39m IntervalStrategy\u001b[38;5;241m.\u001b[39mSTEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.STEPS"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b9df5-17fd-4df6-8028-dc88413c712b",
   "metadata": {},
   "source": [
    "# Paso 9: Entrenamiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a7162-9ed8-48b5-addf-e836334935c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer( \n",
    "\n",
    "    model=model, \n",
    "\n",
    "    args=training_args, \n",
    "\n",
    "    train_dataset=train_dataset, \n",
    "\n",
    "    eval_dataset=test_dataset, \n",
    "\n",
    "    compute_metrics=compute_metrics \n",
    "\n",
    ") \n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05cee1-e667-44cc-9127-feb88c323221",
   "metadata": {},
   "source": [
    "# Paso 10: Evaluación final del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023e410-255d-4e28-b8ff-452631e84a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate() \n",
    "\n",
    "print(\"Resultados:\", results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4878e7-6a66-490f-b8c1-7cf5e6a5e070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
